# MultiModal Learning Research

MultiModal Learning(MML) refers to models that can learn from multiple modalities such as vision, language, robotic actions, etc...MML is a hot area in AI research. AI systems that learn from single modality have advanced rapidly in recent times. We have witnessed language models that can understand texts, image models that can recognize images, etc...Although those systems are not perfect yet, it's fair to say that "learning from single modality" is solved to some extent. A key challenge now is how to design AI systems that can jointly learn and generalize across multiple modalities. The kind of systems that can understand, texts, robotic actions, etc...

This repository is a collection of progress happening in MultiModal Learning. It features lecture videos, papers, books, blog posts, and APIs.

## Papers




## Lecture Videos




## Books




## Blog Posts